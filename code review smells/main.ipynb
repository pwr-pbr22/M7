{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Inicjalizacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from evaluator import evaluate, calc_impact, create_functions_in_db, calc_prs_bugginess, overwrite_bugginess_function, get_considered_prs\n",
    "from definitions import Repository\n",
    "from configuration import ProjectConfiguration\n",
    "import db\n",
    "import smells\n",
    "import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "#plots size in inches\n",
    "plt.rcParams[\"figure.figsize\"] = (10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config = ProjectConfiguration()\n",
    "\n",
    "db.prepare(config.connstr)\n",
    "\n",
    "dbsession = db.get_session()\n",
    "create_functions_in_db(dbsession)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Ewaluacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "repositories = list(map(lambda repository_name: dbsession.query(Repository).filter(Repository.full_name == repository_name).first(), config.projects))\n",
    "\n",
    "if None in repositories:\n",
    "    raise LookupError(\"One of repositories does not exist in the database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Udział smelly prs wśród badanych prs (reprodukcja)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "chosen_simple_tests = [\n",
    "    smells.lack_of_review,\n",
    "    smells.missing_description,\n",
    "    smells.large_changesets,\n",
    "    smells.sleeping_reviews,\n",
    "    smells.review_buddies,\n",
    "    smells.ping_pong\n",
    "]\n",
    "chosen_complex_tests = [\n",
    "    (smells.union, [smells.lack_of_review,\n",
    "                    smells.missing_description,\n",
    "                    smells.large_changesets,\n",
    "                    smells.sleeping_reviews,\n",
    "                    smells.review_buddies,\n",
    "                    smells.ping_pong]),\n",
    "    (smells.intersection, [smells.lack_of_review,\n",
    "                    smells.missing_description,\n",
    "                    smells.large_changesets,\n",
    "                    smells.sleeping_reviews,\n",
    "                    smells.review_buddies,\n",
    "                    smells.ping_pong])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "smells_evaluations = {}\n",
    "\n",
    "for repository in repositories:\n",
    "    tests_results = list(map(lambda simple_test: evaluate(repository.full_name, simple_test), chosen_simple_tests))\n",
    "    tests_results.extend(list(map(lambda complex_test: evaluate(repository.full_name, complex_test[0], complex_test[1]), chosen_complex_tests)))\n",
    "    smells_evaluations[repository.full_name] = tests_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# display results as a text\n",
    "print(\"Smell / repository\".ljust(30), end=\"\\t\")\n",
    "column_width=max(len(repo.name) for repo in repositories)+1\n",
    "print(*map(lambda repo: repo.name.ljust(column_width), repositories),sep=\"\\t\")\n",
    "\n",
    "for i in range(0, len(chosen_simple_tests)):\n",
    "    print(next(iter(smells_evaluations.values()))[i].evaluator_name.ljust(30), end=\"\\t\")\n",
    "    print(*map(lambda tests_results: f\"{round(tests_results[i].percentage*100,2)}%\".rjust(column_width), smells_evaluations.values()), sep=\"\\t\")\n",
    "\n",
    "for i in range(len(chosen_simple_tests), len(chosen_simple_tests)+len(chosen_complex_tests)):\n",
    "    print(next(iter(smells_evaluations.values()))[i].evaluator_name.ljust(30), end=\"\\t\")\n",
    "    print(*map(lambda tests_results: f\"{round(tests_results[i].percentage*100,2)}%\".rjust(column_width), smells_evaluations.values()), sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# display results as a plot\n",
    "labels = list(map(lambda eval: eval.evaluator_name, next(iter(smells_evaluations.values()))))\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "counter = 0.5\n",
    "for repository in smells_evaluations:\n",
    "    bar = ax.bar(x - width/2 + width/len(smells_evaluations)*counter, list(map(lambda evaluation: evaluation.percentage, smells_evaluations[repository])), width/len(smells_evaluations), label=repository)\n",
    "    counter+=1\n",
    "ax.set_xticks(x, labels, rotation=\"vertical\")\n",
    "ax.set_ylim([0, 1])\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Wpływ smelli na prawdopodobieństwo wprowadzenia błędu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tests_for_impact_evaluation = [\n",
    "    (\"Lack of code review\", smells.lack_of_review),\n",
    "    (\"Missing PR description\", smells.missing_description),\n",
    "    (\"Large changeset\", smells.large_changesets),\n",
    "    (\"Sleeping reviews\", smells.sleeping_reviews),\n",
    "    (\"Review Buddies\", smells.review_buddies),\n",
    "    (\"Ping-pong reviews\", smells.ping_pong)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "impact_evaluations = {}\n",
    "\n",
    "for repository in repositories:\n",
    "    print(f\"[{repository.full_name}] Started impact evaluation\")\n",
    "    impact_evaluations[repository.full_name] = list(map(lambda pair: (pair[0], calc_impact(session = dbsession,\n",
    "                                                                 repo = dbsession.get(Repository, repository.id),\n",
    "                                                                 evaluator = pair[1],\n",
    "                                                                 evaluator_args=None)), tests_for_impact_evaluation))\n",
    "    print(f\"[{repository.full_name}] Finished impact evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# display results as text\n",
    "print(\n",
    "    f\"Percentage of pulls where at least one file was changed next by bug solving PR:\")\n",
    "\n",
    "print(\"Smell / repository\".ljust(30), end=\"\\t\")\n",
    "column_width = max(column_width, 22)\n",
    "print(*map(lambda repo: repo.name.ljust(column_width), repositories),sep=\"\\t\")\n",
    "\n",
    "for i in range(0, len(tests_for_impact_evaluation)):\n",
    "    print(next(iter(impact_evaluations.values()))[i][0].ljust(30), end=\"\\t\")\n",
    "    print(*map(lambda tests_results: (f\"+{round(tests_results[i][1][0]*100,1)}% \".rjust(7)+\n",
    "                                     f\"-{round(tests_results[i][1][1]*100,1)}% \".rjust(7)+\"Δ=\"+\n",
    "                                     (('+' if tests_results[i][1][1]>tests_results[i][1][0] else '')+\n",
    "                                     f\"{round((tests_results[i][1][1]-tests_results[i][1][0])*100,1)}%\").rjust(6)).rjust(column_width), impact_evaluations.values()), sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# display results as a plot\n",
    "labels = list(map(lambda eval: eval[0], next(iter(impact_evaluations.values()))))\n",
    "x = np.arange(len(labels))\n",
    "\n",
    "width=0.35\n",
    "fig, ax = plt.subplots()\n",
    "counter = 0.5\n",
    "for repository in impact_evaluations:\n",
    "    bar = ax.bar(x - width/2 + width/len(impact_evaluations)*counter, list(map(lambda evaluation: evaluation[1][1]-evaluation[1][0], impact_evaluations[repository])), width/len(smells_evaluations), label=repository)\n",
    "    counter+=1\n",
    "ax.set_xticks(x, labels, rotation=\"vertical\")\n",
    "ax.set_ylim([-1, 1])\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Metryki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Funkcja zliczająca metrykę bugginess może zostać częściowo nadpisana, domyślnie jest ona następująca:\n",
    "$\n",
    "\\left\\{\\begin{array}{ll}\n",
    "    f(0)=0 &\\\\\n",
    "    f(n)=f(x_{n-1})+\\frac{n^{-2}}{\\text{number of files in original PR}} &\\text{gdy n-ty PR zmieniający plik naprawia defektu}\\\\\n",
    "    f(n)=f(n-1) &\\text{gdy n-ty PR zmieniający plik nie naprawia defektu}\n",
    "  \\end{array}\\right.\n",
    "$\n",
    "\n",
    "Można nadpisać środkowe równanie.\n",
    "\n",
    "Domyślnie $n\\in[0; 4]\\cap\\mathbb{N}$, jednak głębokość przeszukiwania również może zostać nadpisana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# bugginess function can be overwritten, below current expression\n",
    "# overwrite_bugginess_function(dbsession, \"res + pow(i,-2)/number_of_files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "calculated_metrics = [\n",
    "    metrics.review_window_metric,\n",
    "    metrics.review_window_per_line_metric#,\n",
    "    # metrics.review_chars,\n",
    "    # metrics.review_chars_code_lines_ratio,\n",
    "    # metrics.reviewed_lines_per_hour\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "metrics_evaluations = {}\n",
    "\n",
    "for repository in repositories:\n",
    "    print(f\"[{repository.full_name}] Started metrics evaluation\")\n",
    "    for metric in calculated_metrics:\n",
    "        print(f\"[{repository.full_name}] Calculating {metric.__name__}\")\n",
    "        metrics_evaluations[repository.full_name] = metrics_evaluations.get(repository.full_name, [])\n",
    "        metrics_evaluations[repository.full_name].append(evaluate(repository.full_name, metric))\n",
    "    metrics_evaluations[repository.full_name] = metrics_evaluations.get(repository.full_name, [])\n",
    "    metrics_evaluations[repository.full_name].append(evaluate(repository.full_name, calc_prs_bugginess))\n",
    "    # can be changed for depth other than 4 (below no limit)\n",
    "    # metrics_evaluations[repository.full_name] = metrics_evaluations.get(repository.full_name, [])\n",
    "    # metrics_evaluations[repository.full_name].append(calc_prs_bugginess(repository, get_considered_prs(repository,db.get_session()), depth=None))\n",
    "    print(f\"[{repository.full_name}] Finished metrics evaluation\")\n",
    "\n",
    "# create dataframes\n",
    "metrics_evaluations_df = {}\n",
    "for repository in repositories:\n",
    "    df = pd.DataFrame()\n",
    "    for m in metrics_evaluations[repository.full_name]:\n",
    "        df[m.metric_name] = m.to_list(dbsession)\n",
    "    metrics_evaluations_df[repository.full_name] = df\n",
    "\n",
    "overall_metrics_df = pd.concat(metrics_evaluations_df.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# fig = plt.figure()\n",
    "# ax = fig.add_axes([0,0,1,1])\n",
    "# ax.boxplot(list(map(lambda m: m.to_list(dbsession), metrics_evaluations)),\n",
    "#     showfliers=False, notch=True, vert=False)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# calculate correlation\n",
    "correlations = {}\n",
    "for repository in repositories:\n",
    "    correlations[repository.full_name] = metrics_evaluations_df[repository.full_name].corr()\n",
    "overall_correlations = overall_metrics_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# display correlation matrix\n",
    "for repository in repositories:\n",
    "    print(f\"Correlation for repository {repository.full_name}\")\n",
    "    print(correlations[repository.full_name])\n",
    "    print()\n",
    "print(f\"Correlation for all repositories\")\n",
    "print(overall_correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# display correlation heatmaps\n",
    "for repository in repositories:\n",
    "    hm = sns.heatmap(correlations[repository.full_name], annot = True)\n",
    "    hm.set(title = f\"Correlation matrix of reviews metrics and bugginess metrics for {repository.full_name}\\n\")\n",
    "    plt.show()\n",
    "hm = sns.heatmap(overall_correlations, annot = True)\n",
    "hm.set(title = f\"Correlation matrix of reviews metrics and bugginess metrics for all repositories\\n\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# display plots of bugginess against each metrics\n",
    "# overall_metrics_df = pd.concat(metrics_evaluations_df.values())\n",
    "# upper_limit = overall_metrics_df.quantile(0.95)\n",
    "# fig, axs = plt.subplots(len(df.columns)-1)\n",
    "#\n",
    "# index = 0\n",
    "# for column_name in overall_metrics_df.columns[:-1]:\n",
    "#     col = getattr(overall_metrics_df, column_name)\n",
    "#     for repository in repositories:\n",
    "#         df = metrics_evaluations_df[repository.full_name]\n",
    "#         xs = df[(col < upper_limit[column_name]) & (df.bugginess < upper_limit[\"bugginess\"])][column_name]\n",
    "#         ys = df[(col < upper_limit[column_name]) & (df.bugginess < upper_limit[\"bugginess\"])][\"bugginess\"]\n",
    "#         axs[index].plot(xs, ys, \"ro\")\n",
    "#     axs[index].set_xlabel(f\"{column_name}\\nommited {len(df)-len(xs)} of {len(df)} prs\")\n",
    "#     axs[index].set_ylabel(\"bugginess\")\n",
    "#     index += 1\n",
    "#\n",
    "# plt.subplots_adjust(top=1, bottom=0, hspace=0.25, wspace=1)\n",
    "# fig.set_figheight((len(df.columns)-1)*fig.get_figheight())\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}