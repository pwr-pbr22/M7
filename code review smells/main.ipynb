{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialization\n",
    "%matplotlib widget\n",
    "\n",
    "from evaluator import evaluate, get_considered_prs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import tree\n",
    "from definitions import Repository, PullRequest, Commit\n",
    "from configuration import ProjectConfiguration\n",
    "import db\n",
    "import smells\n",
    "import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import ipywidgets as widgets\n",
    "import itertools\n",
    "from sqlalchemy.orm import joinedload, subqueryload\n",
    "from sqlalchemy import select, column\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "\n",
    "#plots size in inches\n",
    "plt.rcParams[\"figure.figsize\"] = (13,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = ProjectConfiguration()\n",
    "\n",
    "db.prepare(config.connstr)\n",
    "dbsession = db.get_session()\n",
    "\n",
    "repositories = list(map(lambda repository_name: dbsession.query(Repository).filter(Repository.full_name == repository_name).first(), config.projects))\n",
    "if None in repositories:\n",
    "    raise LookupError(\"One of repositories does not exist in the database\")\n",
    "repositories = list(filter(lambda repo: len(get_considered_prs(repo, dbsession).all())>0, repositories))\n",
    "    \n",
    "def avg(lst):\n",
    "    return sum(lst) / len(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model based solely on data from _A Large Dataset for Just-In-Time Defect Prediction_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data preparation\n",
    "df = pd.DataFrame(columns=[\"pull_id\",\"buggy\",\"la_min\",\"la_avg\",\"la_max\",\"ld_min\",\"ld_avg\",\"ld_max\",\"nf_min\",\"nf_avg\",\"nf_max\",\"nd_min\",\"nd_avg\",\"nd_max\",\"ns_min\",\"ns_avg\",\"ns_max\",\"ent_min\",\"ent_avg\",\"ent_max\",\"ndev_min\",\"ndev_avg\",\"ndev_max\",\"age_min\",\"age_avg\",\"age_max\",\"nuc_min\",\"nuc_avg\",\"nuc_max\",\"aexp_min\",\"aexp_avg\",\"aexp_max\",\"arexp_min\",\"arexp_avg\",\"arexp_max\",\"asexp_min\",\"asexp_avg\",\"asexp_max\"])\n",
    "for repo in repositories:\n",
    "    prs = get_considered_prs(repo, dbsession).all()\n",
    "    repo_df = pd.concat(list(map(lambda pr: \n",
    "                                 pd.DataFrame({\n",
    "                                     \"pull_id\": [pr.id],\n",
    "                                     \"buggy\": [any(map(lambda c: c.buggy, pr.commits))],\n",
    "                                     \"la_min\": [min(list(map(lambda c: c.la, pr.commits)))],\n",
    "                                     \"la_avg\": [stats.trim_mean(list(map(lambda c: c.la, pr.commits)),0.1)],\n",
    "                                     \"la_max\": [max(list(map(lambda c: c.la, pr.commits)))],\n",
    "                                     \"ld_min\": [min(list(map(lambda c: c.ld, pr.commits)))],\n",
    "                                     \"ld_avg\": [stats.trim_mean(list(map(lambda c: c.ld, pr.commits)),0.1)],\n",
    "                                     \"ld_max\": [max(list(map(lambda c: c.ld, pr.commits)))],\n",
    "                                     \"nf_min\": [min(list(map(lambda c: c.nf, pr.commits)))],\n",
    "                                     \"nf_avg\": [stats.trim_mean(list(map(lambda c: c.nf, pr.commits)),0.1)],\n",
    "                                     \"nf_max\": [max(list(map(lambda c: c.nf, pr.commits)))],\n",
    "                                     \"nd_min\": [min(list(map(lambda c: c.nd, pr.commits)))],\n",
    "                                     \"nd_avg\": [stats.trim_mean(list(map(lambda c: c.nd, pr.commits)),0.1)],\n",
    "                                     \"nd_max\": [max(list(map(lambda c: c.nd, pr.commits)))],\n",
    "                                     \"ns_min\": [min(list(map(lambda c: c.ns, pr.commits)))],\n",
    "                                     \"ns_avg\": [stats.trim_mean(list(map(lambda c: c.ns, pr.commits)),0.1)],\n",
    "                                     \"ns_max\": [max(list(map(lambda c: c.ns, pr.commits)))],\n",
    "                                     \"ent_min\": [min(list(map(lambda c: c.ent, pr.commits)))],\n",
    "                                     \"ent_avg\": [stats.trim_mean(list(map(lambda c: c.ent, pr.commits)),0.1)],\n",
    "                                     \"ent_max\": [max(list(map(lambda c: c.ent, pr.commits)))],\n",
    "                                     \"ndev_min\": [min(list(map(lambda c: c.ndev, pr.commits)))],\n",
    "                                     \"ndev_avg\": [stats.trim_mean(list(map(lambda c: c.ndev, pr.commits)),0.1)],\n",
    "                                     \"ndev_max\": [max(list(map(lambda c: c.ndev, pr.commits)))],\n",
    "                                     \"age_min\": [min(list(map(lambda c: c.age, pr.commits)))],\n",
    "                                     \"age_avg\": [stats.trim_mean(list(map(lambda c: c.age, pr.commits)),0.1)],\n",
    "                                     \"age_max\": [max(list(map(lambda c: c.age, pr.commits)))],\n",
    "                                     \"nuc_min\": [min(list(map(lambda c: c.nuc, pr.commits)))],\n",
    "                                     \"nuc_avg\": [stats.trim_mean(list(map(lambda c: c.nuc, pr.commits)),0.1)],\n",
    "                                     \"nuc_max\": [max(list(map(lambda c: c.nuc, pr.commits)))],\n",
    "                                     \"aexp_min\": [min(list(map(lambda c: c.aexp, pr.commits)))],\n",
    "                                     \"aexp_avg\": [stats.trim_mean(list(map(lambda c: c.aexp, pr.commits)),0.1)],\n",
    "                                     \"aexp_max\": [max(list(map(lambda c: c.aexp, pr.commits)))],\n",
    "                                     \"arexp_min\": [min(list(map(lambda c: c.arexp, pr.commits)))],\n",
    "                                     \"arexp_avg\": [stats.trim_mean(list(map(lambda c: c.arexp, pr.commits)),0.1)],\n",
    "                                     \"arexp_max\": [max(list(map(lambda c: c.arexp, pr.commits)))],\n",
    "                                     \"asexp_min\": [min(list(map(lambda c: c.asexp, pr.commits)))],\n",
    "                                     \"asexp_avg\": [stats.trim_mean(list(map(lambda c: c.asexp, pr.commits)),0.1)],\n",
    "                                     \"asexp_max\": [max(list(map(lambda c: c.asexp, pr.commits)))]\n",
    "                                 }),\n",
    "                            prs)))\n",
    "    df = pd.concat([df, repo_df])\n",
    "df = df.set_index(\"pull_id\")\n",
    "labels = np.array(df[\"buggy\"])\n",
    "features = df.drop(\"buggy\", axis = 1)\n",
    "feature_list = list(features.columns)\n",
    "features = np.array(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selected training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Preparing training and testing sets\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 0)\n",
    "\n",
    "print('Training features shape:\\t', train_features.shape)\n",
    "print('Training labels shape:\\t\\t', train_labels.shape)\n",
    "print('Testing features shape:\\t\\t', test_features.shape)\n",
    "print('Testing labels shape:\\t\\t', test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "rf = RandomForestRegressor(random_state = 0)\n",
    "rf.fit(train_features, train_labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Predicting\n",
    "predictions = rf.predict(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Calculatemean absolute error\n",
    "errors = abs(predictions - test_labels)\n",
    "print(f\"Mean absolute error:\\t\\t{round(np.mean(errors), 2)}\")\n",
    "errors = np.mean((predictions - test_labels)**2)\n",
    "print(f\"Mean squared error:\\t\\t{round(np.mean(errors), 2)}\")\n",
    "print(f\"Root Mean squared error:\\t{round(np.mean(errors)**0.5, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model based on our metrics and smells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Smells\n",
    "### Share of smelly pulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "chosen_simple_tests = [\n",
    "    smells.lack_of_review,\n",
    "    smells.missing_description,\n",
    "    #smells.large_changesets,\n",
    "    smells.sleeping_reviews,\n",
    "    smells.review_buddies,\n",
    "    smells.ping_pong\n",
    "]\n",
    "chosen_complex_tests = [\n",
    "    (smells.union, [smells.lack_of_review,\n",
    "                    smells.missing_description,\n",
    "                    smells.large_changesets,\n",
    "                    smells.sleeping_reviews,\n",
    "                    smells.review_buddies,\n",
    "                    smells.ping_pong]),\n",
    "    (smells.intersection, [smells.lack_of_review,\n",
    "                    smells.missing_description,\n",
    "                    smells.large_changesets,\n",
    "                    smells.sleeping_reviews,\n",
    "                    smells.review_buddies,\n",
    "                    smells.ping_pong])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "smells_evaluations = {}\n",
    "\n",
    "for repository in repositories:\n",
    "    tests_results = list(map(lambda simple_test: evaluate(repository.full_name, simple_test), chosen_simple_tests))\n",
    "    tests_results.extend(list(map(lambda complex_test: evaluate(repository.full_name, complex_test[0], complex_test[1]), chosen_complex_tests)))\n",
    "    smells_evaluations[repository.full_name] = tests_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# display results as a text\n",
    "print(\"Smell / repository\".ljust(30), end=\"\\t\")\n",
    "column_width=max(len(repo.name) for repo in repositories)+1\n",
    "print(*map(lambda repo: repo.name.ljust(column_width), repositories),sep=\"\\t\")\n",
    "\n",
    "for i in range(0, len(chosen_simple_tests)):\n",
    "    print(next(iter(smells_evaluations.values()))[i].evaluator_name.ljust(30), end=\"\\t\")\n",
    "    print(*map(lambda tests_results: f\"{round(tests_results[i].percentage*100,2)}%\".rjust(column_width), smells_evaluations.values()), sep=\"\\t\")\n",
    "\n",
    "for i in range(len(chosen_simple_tests), len(chosen_simple_tests)+len(chosen_complex_tests)):\n",
    "    print(next(iter(smells_evaluations.values()))[i].evaluator_name.ljust(30), end=\"\\t\")\n",
    "    print(*map(lambda tests_results: f\"{round(tests_results[i].percentage*100,2)}%\".rjust(column_width), smells_evaluations.values()), sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# display results as a plot\n",
    "labels = list(map(lambda eval: eval.evaluator_name, next(iter(smells_evaluations.values()))))\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "counter = 0.5\n",
    "for repository in smells_evaluations:\n",
    "    bar = ax.bar(x - width/2 + width/len(smells_evaluations)*counter, list(map(lambda evaluation: evaluation.percentage, smells_evaluations[repository])), width/len(smells_evaluations), label=repository)\n",
    "    counter+=1\n",
    "ax.set_xticks(x, labels, rotation=\"vertical\")\n",
    "ax.set_ylim([0, 1])\n",
    "ax.legend(bbox_to_anchor =(1, 1))\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "calculated_metrics = [\n",
    "    metrics.review_window_metric,\n",
    "    metrics.review_window_per_line_metric,\n",
    "    metrics.review_chars,\n",
    "    metrics.review_chars_code_lines_ratio,\n",
    "    metrics.no_of_reviewers,\n",
    "    metrics.no_of_reviewers_diff_than_author,\n",
    "    metrics.reviewed_lines_per_hour,\n",
    "    metrics.no_of_reviews,\n",
    "    metrics.ping_pong\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics_evaluations = {}\n",
    "\n",
    "for repository in repositories:\n",
    "    for metric in calculated_metrics:\n",
    "        metrics_evaluations[repository.full_name] = metrics_evaluations.get(repository.full_name, [])\n",
    "        metrics_evaluations[repository.full_name].append(evaluate(repository.full_name, metric))\n",
    "    metrics_evaluations[repository.full_name] = metrics_evaluations.get(repository.full_name, [])\n",
    "\n",
    "# create dataframes\n",
    "metrics_evaluations_df = {}\n",
    "for repository in repositories:\n",
    "    df = pd.DataFrame()\n",
    "    df[\"pull_id\"] = list(map(lambda r: float(r[0]) if r[0] is not None else None, dbsession.execute(select(column(\"id\")).select_from(metrics_evaluations[repository.full_name][0].evaluated.subquery())).all()))\n",
    "    for m in metrics_evaluations[repository.full_name]:\n",
    "        df[m.metric_name] = m.to_list(dbsession)\n",
    "        \n",
    "    # add buggy\n",
    "    df_buggy = pd.DataFrame()\n",
    "    df_buggy[\"buggy\"] = False\n",
    "    for index, row in df.iterrows():\n",
    "        df_buggy = pd.concat([df_buggy, pd.DataFrame({\"buggy\": [any(list(map(lambda commit: commit.buggy, dbsession.query(PullRequest).get(row[\"pull_id\"]).commits)))]}, [index])])\n",
    "    df = df.join(df_buggy[\"buggy\"])\n",
    "    \n",
    "    metrics_evaluations_df[repository.full_name] = df\n",
    "\n",
    "\n",
    "overall_metrics_df = pd.concat(metrics_evaluations_df.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# calculate correlation\n",
    "def calc_correlations(method):\n",
    "    correlations = {}\n",
    "    for repository in repositories:\n",
    "        correlations[repository.full_name] = metrics_evaluations_df[repository.full_name].drop(\"pull_id\", axis = 1).corr(method=method)\n",
    "    overall_correlations = overall_metrics_df.drop(\"pull_id\", axis = 1).corr(method=method)\n",
    "    return correlations, overall_correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation between metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# display correlation heatmaps\n",
    "@widgets.interact(corr_method=[\"pearson\", \"kendall\", \"spearman\"],per_project=False)\n",
    "def display_corr_plots(corr_method=\"spearman\",per_project=False):\n",
    "    correlations, overall_correlations = calc_correlations(corr_method)\n",
    "    if per_project:\n",
    "        for repository in repositories:\n",
    "            fig, ax = plt.subplots()\n",
    "            hm = sns.heatmap(correlations[repository.full_name], annot = True, ax=ax)\n",
    "            hm.set(title = f\"Correlation matrix of reviews metrics and bugginess for {repository.full_name}\\n\")\n",
    "            fig.subplots_adjust(left=0.2, bottom=0.4)\n",
    "            ax.set_xticklabels(list(map(lambda label: label.get_text().replace('_',' '), ax.get_xticklabels())),rotation=\"vertical\")\n",
    "            ax.set_yticklabels(list(map(lambda label: label.get_text().replace('_',' '), ax.get_yticklabels())))\n",
    "            plt.show()\n",
    "    fig, ax = plt.subplots()\n",
    "    hm = sns.heatmap(overall_correlations, annot = True, ax=ax)\n",
    "    hm.set(title = f\"Correlation matrix of reviews metrics and bugginess for all repositories\\n\")\n",
    "    fig.subplots_adjust(left=0.2, bottom=0.4)\n",
    "    ax.set_xticklabels(list(map(lambda label: label.get_text().replace('_',' '), ax.get_xticklabels())),rotation=\"vertical\")\n",
    "    ax.set_yticklabels(list(map(lambda label: label.get_text().replace('_',' '), ax.get_yticklabels())))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplots for different metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# display boxplots\n",
    "@widgets.interact(outlier=False,per_project=False)\n",
    "def display_boxplots(outlier,per_project):\n",
    "    for column_name in overall_metrics_df.columns[1:-1]:\n",
    "        if per_project:\n",
    "            for repository in repositories:\n",
    "                    fig, ax = plt.subplots()\n",
    "                    df = metrics_evaluations_df[repository.full_name]\n",
    "                    ax.boxplot([df[df.buggy == 0][column_name], df[df.buggy == 1][column_name]], 0, 'k+' if outlier else '')\n",
    "                    ax.set_xticklabels([\"nonbuggy\", \"buggy\"], fontsize=8)\n",
    "                    ax.set_title(f\"Boxplot for {column_name.replace('_',' ')} in repository {repository.full_name}\")\n",
    "                    plt.show()\n",
    "        fig, ax = plt.subplots()\n",
    "        df = overall_metrics_df\n",
    "        col = getattr(df, column_name)\n",
    "        ax.boxplot([df[df.buggy == 0][column_name], df[df.buggy == 1][column_name]], 0, 'k+' if outlier else '')\n",
    "        ax.set_xticklabels([\"nonbuggy\", \"buggy\"], fontsize=8)\n",
    "        ax.set_title(f\"Boxplot for {column_name.replace('_',' ')} in all repositories\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = overall_metrics_df.copy()\n",
    "labels = np.array(df[\"buggy\"])\n",
    "features = df.drop(\"buggy\", axis = 1)\n",
    "smells_results = {}\n",
    "for smell in chosen_simple_tests:\n",
    "    evaluations = map(lambda repository: evaluate(repository.full_name, smell), repositories)\n",
    "    considered = list(map(lambda pr: pr.id, itertools.chain(*map(lambda e: e.considered.all(), evaluations))))\n",
    "    evaluations = map(lambda repository: evaluate(repository.full_name, smell), repositories)\n",
    "    smelly = list(map(lambda pr: pr.id, itertools.chain(*map(lambda e: e.smelly.all(), evaluations))))\n",
    "    smells_results[smell.__name__] = (considered, smelly)\n",
    "\n",
    "for smell in smells_results:\n",
    "    temp_df = pd.DataFrame({smell: list(map(lambda considered: considered in smells_results[smell][1], smells_results[smell][0])),\n",
    "                            \"pull_id\": considered})\n",
    "    features = features.join(temp_df.set_index(\"pull_id\"), on=\"pull_id\", lsuffix=\"_metric\", rsuffix=\"_smell\")\n",
    "features = features.set_index(\"pull_id\")\n",
    "feature_list = list(features.columns)\n",
    "features = np.array(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Selected training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Preparing training and testing sets\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 0)\n",
    "\n",
    "print('Training features shape:\\t', train_features.shape)\n",
    "print('Training labels shape:\\t\\t', train_labels.shape)\n",
    "print('Testing features shape:\\t\\t', test_features.shape)\n",
    "print('Testing labels shape:\\t\\t', test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# training\n",
    "rf = RandomForestRegressor(random_state = 0)\n",
    "rf.fit(train_features, train_labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Use the forest's predict method on the test data\n",
    "predictions = rf.predict(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Prediction errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Calculatemean absolute error\n",
    "errors = abs(predictions - test_labels)\n",
    "print(f\"Mean absolute error:\\t\\t{round(np.mean(errors), 2)}\")\n",
    "errors = np.mean((predictions - test_labels)**2)\n",
    "print(f\"Mean squared error:\\t\\t{round(np.mean(errors), 2)}\")\n",
    "print(f\"Root Mean squared error:\\t{round(np.mean(errors)**0.5, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Metrics importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get numerical feature importances\n",
    "importances = list(rf.feature_importances_)\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "# Print out the feature and importances\n",
    "print(\"Metrics                        Importance\")\n",
    "[print(f\"{pair[0].replace('_',' ').ljust(30,' ')} {pair[1]}\") for pair in feature_importances];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.bar(feature_list, importances)\n",
    "ax.set_xticklabels(list(map(lambda e: e.replace('_',' '), feature_list)),rotation=\"vertical\")\n",
    "ax.set_ylim(0,1)\n",
    "fig.subplots_adjust( bottom=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Example of a tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pull out one tree from the forest\n",
    "@widgets.interact_manual(tree_number=widgets.IntSlider(min=0, max=rf.get_params(deep=False)[\"n_estimators\"]-1, step=1, value=0), max_depth=widgets.IntSlider(min=0, max=30, step=1, value=2))\n",
    "def plot_tree(tree_number, max_depth):\n",
    "    _, ax = plt.subplots()\n",
    "    tree.plot_tree(rf.estimators_[tree_number],\n",
    "                   feature_names = feature_list,\n",
    "                   class_names = labels,\n",
    "                   filled = True,\n",
    "                   ax = ax,\n",
    "                   max_depth=max_depth)\n",
    "    ax.text(0,1, f\"depth = {rf.estimators_[tree_number].get_depth()}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "features = overall_metrics_df.copy()\n",
    "smells_results = {}\n",
    "for smell in chosen_simple_tests:\n",
    "    evaluations = map(lambda repository: evaluate(repository.full_name, smell), repositories)\n",
    "    considered = list(map(lambda pr: pr.id, itertools.chain(*map(lambda e: e.considered.all(), evaluations))))\n",
    "    evaluations = map(lambda repository: evaluate(repository.full_name, smell), repositories)\n",
    "    smelly = list(map(lambda pr: pr.id, itertools.chain(*map(lambda e: e.smelly.all(), evaluations))))\n",
    "    smells_results[smell.__name__] = (considered, smelly)\n",
    "\n",
    "for smell in smells_results:\n",
    "    temp_df = pd.DataFrame({smell: list(map(lambda considered: considered in smells_results[smell][1], smells_results[smell][0])),\n",
    "                            \"pull_id\": considered})\n",
    "    features = features.join(temp_df.set_index(\"pull_id\"), on=\"pull_id\", lsuffix=\"_metric\", rsuffix=\"_smell\")\n",
    "features = features.set_index(\"pull_id\")\n",
    "\n",
    "\n",
    "df = pd.DataFrame(columns=[\"pull_id\",\"la_min\",\"la_avg\",\"la_max\",\"ld_min\",\"ld_avg\",\"ld_max\",\"nf_min\",\"nf_avg\",\"nf_max\",\"nd_min\",\"nd_avg\",\"nd_max\",\"ns_min\",\"ns_avg\",\"ns_max\",\"ent_min\",\"ent_avg\",\"ent_max\",\"ndev_min\",\"ndev_avg\",\"ndev_max\",\"age_min\",\"age_avg\",\"age_max\",\"nuc_min\",\"nuc_avg\",\"nuc_max\",\"aexp_min\",\"aexp_avg\",\"aexp_max\",\"arexp_min\",\"arexp_avg\",\"arexp_max\",\"asexp_min\",\"asexp_avg\",\"asexp_max\"])\n",
    "for repo in repositories:\n",
    "    prs = get_considered_prs(repo, dbsession).all()\n",
    "    repo_df = pd.concat(list(map(lambda pr: \n",
    "                                 pd.DataFrame({\n",
    "                                     \"pull_id\": [pr.id],\n",
    "                                     \"la_min\": [min(list(map(lambda c: c.la, pr.commits)))],\n",
    "                                     \"la_avg\": [stats.trim_mean(list(map(lambda c: c.la, pr.commits)),0.1)],\n",
    "                                     \"la_max\": [max(list(map(lambda c: c.la, pr.commits)))],\n",
    "                                     \"ld_min\": [min(list(map(lambda c: c.ld, pr.commits)))],\n",
    "                                     \"ld_avg\": [stats.trim_mean(list(map(lambda c: c.ld, pr.commits)),0.1)],\n",
    "                                     \"ld_max\": [max(list(map(lambda c: c.ld, pr.commits)))],\n",
    "                                     \"nf_min\": [min(list(map(lambda c: c.nf, pr.commits)))],\n",
    "                                     \"nf_avg\": [stats.trim_mean(list(map(lambda c: c.nf, pr.commits)),0.1)],\n",
    "                                     \"nf_max\": [max(list(map(lambda c: c.nf, pr.commits)))],\n",
    "                                     \"nd_min\": [min(list(map(lambda c: c.nd, pr.commits)))],\n",
    "                                     \"nd_avg\": [stats.trim_mean(list(map(lambda c: c.nd, pr.commits)),0.1)],\n",
    "                                     \"nd_max\": [max(list(map(lambda c: c.nd, pr.commits)))],\n",
    "                                     \"ns_min\": [min(list(map(lambda c: c.ns, pr.commits)))],\n",
    "                                     \"ns_avg\": [stats.trim_mean(list(map(lambda c: c.ns, pr.commits)),0.1)],\n",
    "                                     \"ns_max\": [max(list(map(lambda c: c.ns, pr.commits)))],\n",
    "                                     \"ent_min\": [min(list(map(lambda c: c.ent, pr.commits)))],\n",
    "                                     \"ent_avg\": [stats.trim_mean(list(map(lambda c: c.ent, pr.commits)),0.1)],\n",
    "                                     \"ent_max\": [max(list(map(lambda c: c.ent, pr.commits)))],\n",
    "                                     \"ndev_min\": [min(list(map(lambda c: c.ndev, pr.commits)))],\n",
    "                                     \"ndev_avg\": [stats.trim_mean(list(map(lambda c: c.ndev, pr.commits)),0.1)],\n",
    "                                     \"ndev_max\": [max(list(map(lambda c: c.ndev, pr.commits)))],\n",
    "                                     \"age_min\": [min(list(map(lambda c: c.age, pr.commits)))],\n",
    "                                     \"age_avg\": [stats.trim_mean(list(map(lambda c: c.age, pr.commits)),0.1)],\n",
    "                                     \"age_max\": [max(list(map(lambda c: c.age, pr.commits)))],\n",
    "                                     \"nuc_min\": [min(list(map(lambda c: c.nuc, pr.commits)))],\n",
    "                                     \"nuc_avg\": [stats.trim_mean(list(map(lambda c: c.nuc, pr.commits)),0.1)],\n",
    "                                     \"nuc_max\": [max(list(map(lambda c: c.nuc, pr.commits)))],\n",
    "                                     \"aexp_min\": [min(list(map(lambda c: c.aexp, pr.commits)))],\n",
    "                                     \"aexp_avg\": [stats.trim_mean(list(map(lambda c: c.aexp, pr.commits)),0.1)],\n",
    "                                     \"aexp_max\": [max(list(map(lambda c: c.aexp, pr.commits)))],\n",
    "                                     \"arexp_min\": [min(list(map(lambda c: c.arexp, pr.commits)))],\n",
    "                                     \"arexp_avg\": [stats.trim_mean(list(map(lambda c: c.arexp, pr.commits)),0.1)],\n",
    "                                     \"arexp_max\": [max(list(map(lambda c: c.arexp, pr.commits)))],\n",
    "                                     \"asexp_min\": [min(list(map(lambda c: c.asexp, pr.commits)))],\n",
    "                                     \"asexp_avg\": [stats.trim_mean(list(map(lambda c: c.asexp, pr.commits)),0.1)],\n",
    "                                     \"asexp_max\": [max(list(map(lambda c: c.asexp, pr.commits)))]\n",
    "                                 }),\n",
    "                            prs)))\n",
    "    df = pd.concat([df, repo_df])\n",
    "df = df.set_index(\"pull_id\")\n",
    "\n",
    "features = features.join(df, how=\"inner\")\n",
    "labels = np.array(features[\"buggy\"])\n",
    "features = features.drop(\"buggy\", axis = 1)\n",
    "feature_list = list(features.columns)\n",
    "features = np.array(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selected training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 0)\n",
    "\n",
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# training\n",
    "rf = RandomForestRegressor(random_state = 0)\n",
    "rf.fit(train_features, train_labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Use the forest's predict method on the test data\n",
    "predictions = rf.predict(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Calculatemean absolute error\n",
    "errors = abs(predictions - test_labels)\n",
    "print(f\"Mean absolute error:\\t\\t{round(np.mean(errors), 2)}\")\n",
    "errors = np.mean((predictions - test_labels)**2)\n",
    "print(f\"Mean squared error:\\t\\t{round(np.mean(errors), 2)}\")\n",
    "print(f\"Root Mean squared error:\\t{round(np.mean(errors)**0.5, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Get numerical feature importances\n",
    "importances = list(rf.feature_importances_)\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "# Print out the feature and importances\n",
    "print(\"Metrics                        Importance\")\n",
    "[print(f\"{pair[0].replace('_',' ').ljust(30,' ')} {pair[1]}\") for pair in feature_importances];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.bar(feature_list, importances)\n",
    "ax.set_xticklabels(list(map(lambda e: e.replace('_',' '), feature_list)),rotation=\"vertical\")\n",
    "ax.set_ylim(0,1)\n",
    "fig.subplots_adjust( bottom=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pull out one tree from the forest\n",
    "@widgets.interact_manual(tree_number=widgets.IntSlider(min=0, max=rf.get_params(deep=False)[\"n_estimators\"]-1, step=1, value=0), max_depth=widgets.IntSlider(min=0, max=30, step=1, value=2))\n",
    "def plot_tree(tree_number, max_depth):\n",
    "    _, ax = plt.subplots()\n",
    "    tree.plot_tree(rf.estimators_[tree_number],\n",
    "                   feature_names = feature_list,\n",
    "                   class_names = labels,\n",
    "                   filled = True,\n",
    "                   ax = ax,\n",
    "                   max_depth=max_depth)\n",
    "    ax.text(0,1, f\"depth = {rf.estimators_[tree_number].get_depth()}\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
