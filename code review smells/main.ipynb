{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Inicjalizacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "from evaluator import evaluate, calc_impact, create_functions_in_db, calc_prs_bugginess, overwrite_bugginess_function, get_considered_prs\n",
    "from definitions import Repository\n",
    "from configuration import ProjectConfiguration\n",
    "import db\n",
    "import smells\n",
    "import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "#plots size in inches\n",
    "plt.rcParams[\"figure.figsize\"] = (9.75,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config = ProjectConfiguration()\n",
    "\n",
    "db.prepare(config.connstr)\n",
    "\n",
    "dbsession = db.get_session()\n",
    "#create_functions_in_db(dbsession)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Ewaluacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "repositories = list(map(lambda repository_name: dbsession.query(Repository).filter(Repository.full_name == repository_name).first(), config.projects))\n",
    "\n",
    "if None in repositories:\n",
    "    raise LookupError(\"One of repositories does not exist in the database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Udział smelly prs wśród badanych prs (reprodukcja)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "chosen_simple_tests = [\n",
    "    smells.lack_of_review,\n",
    "    smells.missing_description,\n",
    "    smells.large_changesets,\n",
    "    smells.sleeping_reviews,\n",
    "    smells.review_buddies,\n",
    "    smells.ping_pong\n",
    "]\n",
    "chosen_complex_tests = [\n",
    "    (smells.union, [smells.lack_of_review,\n",
    "                    smells.missing_description,\n",
    "                    smells.large_changesets,\n",
    "                    smells.sleeping_reviews,\n",
    "                    smells.review_buddies,\n",
    "                    smells.ping_pong]),\n",
    "    (smells.intersection, [smells.lack_of_review,\n",
    "                    smells.missing_description,\n",
    "                    smells.large_changesets,\n",
    "                    smells.sleeping_reviews,\n",
    "                    smells.review_buddies,\n",
    "                    smells.ping_pong])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "smells_evaluations = {}\n",
    "\n",
    "for repository in repositories:\n",
    "    tests_results = list(map(lambda simple_test: evaluate(repository.full_name, simple_test), chosen_simple_tests))\n",
    "    tests_results.extend(list(map(lambda complex_test: evaluate(repository.full_name, complex_test[0], complex_test[1]), chosen_complex_tests)))\n",
    "    smells_evaluations[repository.full_name] = tests_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# display results as a text\n",
    "print(\"Smell / repository\".ljust(30), end=\"\\t\")\n",
    "column_width=max(len(repo.name) for repo in repositories)+1\n",
    "print(*map(lambda repo: repo.name.ljust(column_width), repositories),sep=\"\\t\")\n",
    "\n",
    "for i in range(0, len(chosen_simple_tests)):\n",
    "    print(next(iter(smells_evaluations.values()))[i].evaluator_name.ljust(30), end=\"\\t\")\n",
    "    print(*map(lambda tests_results: f\"{round(tests_results[i].percentage*100,2)}%\".rjust(column_width), smells_evaluations.values()), sep=\"\\t\")\n",
    "\n",
    "for i in range(len(chosen_simple_tests), len(chosen_simple_tests)+len(chosen_complex_tests)):\n",
    "    print(next(iter(smells_evaluations.values()))[i].evaluator_name.ljust(30), end=\"\\t\")\n",
    "    print(*map(lambda tests_results: f\"{round(tests_results[i].percentage*100,2)}%\".rjust(column_width), smells_evaluations.values()), sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# display results as a plot\n",
    "labels = list(map(lambda eval: eval.evaluator_name, next(iter(smells_evaluations.values()))))\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "counter = 0.5\n",
    "for repository in smells_evaluations:\n",
    "    bar = ax.bar(x - width/2 + width/len(smells_evaluations)*counter, list(map(lambda evaluation: evaluation.percentage, smells_evaluations[repository])), width/len(smells_evaluations), label=repository)\n",
    "    counter+=1\n",
    "ax.set_xticks(x, labels, rotation=\"vertical\")\n",
    "ax.set_ylim([0, 1])\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Wpływ smelli na prawdopodobieństwo wprowadzenia błędu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# tests_for_impact_evaluation = [\n",
    "#     (\"Lack of code review\", smells.lack_of_review),\n",
    "#     (\"Missing PR description\", smells.missing_description),\n",
    "#     (\"Large changeset\", smells.large_changesets),\n",
    "#     (\"Sleeping reviews\", smells.sleeping_reviews),\n",
    "#     (\"Review Buddies\", smells.review_buddies),\n",
    "#     (\"Ping-pong reviews\", smells.ping_pong)\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# impact_evaluations = {}\n",
    "\n",
    "# for repository in repositories:\n",
    "#     print(f\"[{repository.full_name}] Started impact evaluation\")\n",
    "#     impact_evaluations[repository.full_name] = list(map(lambda pair: (pair[0], calc_impact(session = dbsession,\n",
    "#                                                                  repo = dbsession.get(Repository, repository.id),\n",
    "#                                                                  evaluator = pair[1],\n",
    "#                                                                  evaluator_args=None)), tests_for_impact_evaluation))\n",
    "#     print(f\"[{repository.full_name}] Finished impact evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # display results as text\n",
    "# print(\n",
    "#     f\"Percentage of pulls where at least one file was changed next by bug solving PR:\")\n",
    "\n",
    "# print(\"Smell / repository\".ljust(30), end=\"\\t\")\n",
    "# column_width = max(column_width, 22)\n",
    "# print(*map(lambda repo: repo.name.ljust(column_width), repositories),sep=\"\\t\")\n",
    "\n",
    "# for i in range(0, len(tests_for_impact_evaluation)):\n",
    "#     print(next(iter(impact_evaluations.values()))[i][0].ljust(30), end=\"\\t\")\n",
    "#     print(*map(lambda tests_results: (f\"+{round(tests_results[i][1][0]*100,1)}% \".rjust(7)+\n",
    "#                                      f\"-{round(tests_results[i][1][1]*100,1)}% \".rjust(7)+\"Δ=\"+\n",
    "#                                      (('+' if tests_results[i][1][1]>tests_results[i][1][0] else '')+\n",
    "#                                      f\"{round((tests_results[i][1][1]-tests_results[i][1][0])*100,1)}%\").rjust(6)).rjust(column_width), impact_evaluations.values()), sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # display results as a plot\n",
    "# labels = list(map(lambda eval: eval[0], next(iter(impact_evaluations.values()))))\n",
    "# x = np.arange(len(labels))\n",
    "\n",
    "# width=0.35\n",
    "# fig, ax = plt.subplots()\n",
    "# counter = 0.5\n",
    "# for repository in impact_evaluations:\n",
    "#     bar = ax.bar(x - width/2 + width/len(impact_evaluations)*counter, list(map(lambda evaluation: evaluation[1][1]-evaluation[1][0], impact_evaluations[repository])), width/len(smells_evaluations), label=repository)\n",
    "#     counter+=1\n",
    "# ax.set_xticks(x, labels, rotation=\"vertical\")\n",
    "# ax.set_ylim([-1, 1])\n",
    "# ax.legend()\n",
    "# plt.gcf().subplots_adjust(bottom=0.35)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Metryki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "calculated_metrics = [\n",
    "    metrics.review_window_metric,\n",
    "    metrics.review_window_per_line_metric\n",
    "    #metrics.review_chars_code_lines_ratio,\n",
    "    #metrics.reviewed_lines_per_hour\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sqlalchemy.orm import joinedload, subqueryload\n",
    "from definitions import PullRequest, Commit\n",
    "from sqlalchemy import select, column\n",
    "\n",
    "metrics_evaluations = {}\n",
    "\n",
    "for repository in repositories:\n",
    "    print(f\"[{repository.full_name}] Preparing queries\")\n",
    "    for metric in calculated_metrics:\n",
    "        metrics_evaluations[repository.full_name] = metrics_evaluations.get(repository.full_name, [])\n",
    "        metrics_evaluations[repository.full_name].append(evaluate(repository.full_name, metric))\n",
    "    metrics_evaluations[repository.full_name] = metrics_evaluations.get(repository.full_name, [])\n",
    "\n",
    "# create dataframes\n",
    "metrics_evaluations_df = {}\n",
    "for repository in repositories:\n",
    "    print(f\"[{repository.full_name}] Creating dataframe\")\n",
    "    df = pd.DataFrame()\n",
    "    df[\"pull_id\"] = list(map(lambda r: float(r[0]) if r[0] is not None else None, dbsession.execute(select(column(\"id\")).select_from(metrics_evaluations[repository.full_name][0].evaluated.subquery())).all()))\n",
    "    for m in metrics_evaluations[repository.full_name]:\n",
    "        print(f\"[{repository.full_name}] Creating dataframe column {m.metric_name}\")\n",
    "        df[m.metric_name] = m.to_list(dbsession)\n",
    "        \n",
    "    # add buggy\n",
    "    df_buggy = pd.DataFrame()\n",
    "    df_buggy[\"buggy\"] = -1\n",
    "    for index, row in df.iterrows():\n",
    "        df_buggy = pd.concat([df_buggy, pd.DataFrame({\"buggy\": [int(any(list(map(lambda commit: commit.buggy, dbsession.query(PullRequest).get(row[\"pull_id\"]).commits))))]}, [index])])\n",
    "    df = df.join(df_buggy[\"buggy\"])\n",
    "    \n",
    "    metrics_evaluations_df[repository.full_name] = df\n",
    "\n",
    "\n",
    "print(\"[All repositories] Creating common dataframe\")\n",
    "overall_metrics_df = pd.concat(metrics_evaluations_df.values())\n",
    "print(f\"[All repositories] Finished creating dataframes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# calculate correlation\n",
    "def calc_correlations(method):\n",
    "    correlations = {}\n",
    "    for repository in repositories:\n",
    "        correlations[repository.full_name] = metrics_evaluations_df[repository.full_name].corr(method=method)\n",
    "    overall_correlations = overall_metrics_df.corr(method=method)\n",
    "    return correlations, overall_correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# display correlation heatmaps\n",
    "@widgets.interact(corr_method=[\"pearson\", \"kendall\", \"spearman\"])\n",
    "def display_corr_plots(corr_method=\"spearman\"):\n",
    "    correlations, overall_correlations = calc_correlations(corr_method)\n",
    "    for repository in repositories:\n",
    "        fig, ax = plt.subplots()\n",
    "        hm = sns.heatmap(correlations[repository.full_name], annot = True, ax=ax)\n",
    "        hm.set(title = f\"Correlation matrix of reviews metrics and bugginess for {repository.full_name}\\n\")\n",
    "        fig.subplots_adjust(left=0.2, bottom=0.4)\n",
    "        ax.set_xticklabels(list(map(lambda label: label.get_text().replace('_',' '), ax.get_xticklabels())),rotation=\"vertical\")\n",
    "        ax.set_yticklabels(list(map(lambda label: label.get_text().replace('_',' '), ax.get_yticklabels())))\n",
    "        plt.show()\n",
    "    fig, ax = plt.subplots()\n",
    "    hm = sns.heatmap(overall_correlations, annot = True, ax=ax)\n",
    "    hm.set(title = f\"Correlation matrix of reviews metrics and bugginess for all repositories\\n\")\n",
    "    fig.subplots_adjust(left=0.2, bottom=0.4)\n",
    "    ax.set_xticklabels(list(map(lambda label: label.get_text().replace('_',' '), ax.get_xticklabels())),rotation=\"vertical\")\n",
    "    ax.set_yticklabels(list(map(lambda label: label.get_text().replace('_',' '), ax.get_yticklabels())))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# display boxplots\n",
    "@widgets.interact(outlier=False)\n",
    "def display_boxplots(outlier):\n",
    "    for column_name in overall_metrics_df.columns[1:-1]:\n",
    "        for repository in repositories:\n",
    "            fig, ax = plt.subplots()\n",
    "            df = metrics_evaluations_df[repository.full_name]\n",
    "            ax.boxplot([df[df.buggy == 0][column_name], df[df.buggy == 1][column_name]], 0, 'k+' if outlier else '')\n",
    "            ax.set_xticklabels([\"nonbuggy\", \"buggy\"], fontsize=8)\n",
    "            ax.set_title(f\"Boxplot for {column_name.replace('_',' ')} in repository {repository.full_name}\")\n",
    "            plt.show()\n",
    "        fig, ax = plt.subplots()\n",
    "        df = overall_metrics_df\n",
    "        col = getattr(df, column_name)\n",
    "        ax.boxplot([df[df.buggy == 0][column_name], df[df.buggy == 1][column_name]], 0, 'k+' if outlier else '')\n",
    "        ax.set_xticklabels([\"nonbuggy\", \"buggy\"], fontsize=8)\n",
    "        ax.set_title(f\"Boxplot for {column_name.replace('_',' ')} in all repositories\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = overall_metrics_df.copy()\n",
    "labels = np.array(df[\"buggy\"])\n",
    "features = df.drop(\"pull_id\", axis = 1)\n",
    "features = features.drop(\"buggy\", axis = 1)\n",
    "feature_list = list(features.columns)\n",
    "features = np.array(features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Preparing training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 42)\n",
    "\n",
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# baseline error should go here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators = 10000, random_state = 42, max_depth=1000)\n",
    "rf.fit(train_features, train_labels);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Use the forest's predict method on the test data\n",
    "predictions = rf.predict(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Performance check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the absolute errors\n",
    "errors = abs(predictions - test_labels)\n",
    "# Print out the mean absolute error (mae)\n",
    "print(f\"Mean Absolute Error: {round(np.mean(errors), 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Metrics importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get numerical feature importances\n",
    "importances = list(rf.feature_importances_)\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "# Print out the feature and importances\n",
    "print(\"Metrics                        Importance\")\n",
    "[print(f\"{pair[0].replace('_',' ').ljust(30,' ')} {pair[1]}\") for pair in feature_importances];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.bar(feature_list, importances)\n",
    "ax.set_xticklabels(feature_list)\n",
    "ax.set_ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Example of a tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "# Pull out one tree from the forest\n",
    "@widgets.interact_manual(tree_number=widgets.IntSlider(min=0, max=rf.get_params(deep=False)[\"n_estimators\"]-1, step=1, value=0), max_depth=widgets.IntSlider(min=0, max=30, step=1, value=2))\n",
    "def plot_tree(tree_number, max_depth):\n",
    "    _, ax = plt.subplots()\n",
    "    tree.plot_tree(rf.estimators_[tree_number],\n",
    "                   feature_names = feature_list,\n",
    "                   class_names = labels,\n",
    "                   filled = True,\n",
    "                   ax = ax,\n",
    "                   max_depth=max_depth)\n",
    "    ax.text(0,1, f\"depth = {rf.estimators_[tree_number].get_depth()}\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}